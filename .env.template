# =============================================================================
# AI AGENT DEVELOPMENT SYSTEM - ENVIRONMENT CONFIGURATION TEMPLATE
# =============================================================================
# Copy this file to .env and fill in your values
# NEVER commit .env to version control
# =============================================================================

# -----------------------------------------------------------------------------
# PROJECT IDENTIFICATION
# -----------------------------------------------------------------------------
# Unique identifier for this project instance
PROJECT_ID=my-project-id

# Human-readable project name
PROJECT_NAME="My AI-Driven Project"

# Environment (development, staging, production)
ENVIRONMENT=development

# -----------------------------------------------------------------------------
# GITHUB INTEGRATION
# -----------------------------------------------------------------------------
# GitHub Personal Access Token or GitHub App token
# Required permissions: repo, issues, write:discussion
GITHUB_TOKEN=ghp_your_github_token_here

# Repository in format: owner/repo
GITHUB_REPO=your-org/your-repo

# GitHub API base URL (change for GitHub Enterprise)
GITHUB_API_URL=https://api.github.com

# Webhook secret for validating incoming webhooks
GITHUB_WEBHOOK_SECRET=your_webhook_secret_here

# GitHub App credentials (alternative to PAT)
GITHUB_APP_ID=
GITHUB_APP_PRIVATE_KEY_PATH=
GITHUB_APP_INSTALLATION_ID=

# -----------------------------------------------------------------------------
# LLM PROVIDER CONFIGURATION
# -----------------------------------------------------------------------------
# Primary LLM provider: openai, anthropic, azure, local
LLM_PROVIDER=anthropic

# API Keys (set the one for your provider)
OPENAI_API_KEY=sk-your_openai_api_key_here
ANTHROPIC_API_KEY=sk-ant-your_anthropic_api_key_here
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=

# Model selection
LLM_MODEL_PLANNER=claude-sonnet-4-20250514
LLM_MODEL_DEVELOPER=claude-sonnet-4-20250514
LLM_MODEL_QA=claude-sonnet-4-20250514
LLM_MODEL_REVIEWER=claude-sonnet-4-20250514
LLM_MODEL_DOC=claude-haiku-4-20250514

# LLM parameters
LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=8192

# -----------------------------------------------------------------------------
# ORCHESTRATOR CONFIGURATION
# -----------------------------------------------------------------------------
# Polling interval in seconds (if not using webhooks)
ORCHESTRATOR_POLL_INTERVAL=30

# Maximum concurrent agent executions
ORCHESTRATOR_MAX_CONCURRENT_AGENTS=3

# State persistence backend: file, redis, postgresql
ORCHESTRATOR_STATE_BACKEND=file

# State file path (for file backend)
ORCHESTRATOR_STATE_FILE=/data/orchestrator_state.json

# Redis configuration (if using redis backend)
REDIS_URL=redis://localhost:6379/0

# PostgreSQL configuration (if using postgresql backend)
DATABASE_URL=postgresql://user:password@localhost:5432/orchestrator

# -----------------------------------------------------------------------------
# AGENT CONFIGURATION
# -----------------------------------------------------------------------------
# Maximum iterations before marking task as BLOCKED
AGENT_MAX_ITERATIONS=5

# Agent execution timeout in seconds
AGENT_TIMEOUT=1800

# Memory path inside agent container
AGENT_MEMORY_PATH=/memory

# Repository path inside agent container
AGENT_REPO_PATH=/repo

# Output path inside agent container
AGENT_OUTPUT_PATH=/output

# -----------------------------------------------------------------------------
# DOCKER CONFIGURATION
# -----------------------------------------------------------------------------
# Docker image for agents
DOCKER_AGENT_IMAGE=ai-agent:latest

# Docker network name
DOCKER_NETWORK=ai-agent-network

# Docker registry (optional, for pulling pre-built images)
DOCKER_REGISTRY=

# Resource limits per agent container
DOCKER_AGENT_CPU_LIMIT=2
DOCKER_AGENT_MEMORY_LIMIT=4g

# -----------------------------------------------------------------------------
# PATHS CONFIGURATION
# -----------------------------------------------------------------------------
# Local path to project memory (mounted to agents)
LOCAL_MEMORY_PATH=./memory

# Local path to code repository (mounted to agents)
LOCAL_REPO_PATH=./repo

# Local path for agent outputs
LOCAL_OUTPUT_PATH=./output

# Logs directory
LOG_PATH=./logs

# -----------------------------------------------------------------------------
# MONITORING AND LOGGING
# -----------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Structured logging format: json, text
LOG_FORMAT=json

# Metrics export endpoint (optional, for Prometheus)
METRICS_ENDPOINT=

# Enable detailed agent execution tracing
ENABLE_TRACING=true

# Trace export endpoint (optional, for Jaeger/Zipkin)
TRACE_ENDPOINT=

# -----------------------------------------------------------------------------
# WEBHOOK SERVER (if running webhook receiver)
# -----------------------------------------------------------------------------
# Port for webhook server
WEBHOOK_PORT=8080

# Webhook server host
WEBHOOK_HOST=0.0.0.0

# External URL for GitHub to send webhooks to
WEBHOOK_EXTERNAL_URL=https://your-domain.com/webhooks/github

# -----------------------------------------------------------------------------
# SECURITY
# -----------------------------------------------------------------------------
# Encryption key for sensitive data at rest
ENCRYPTION_KEY=

# JWT secret for internal API authentication
JWT_SECRET=

# Allowed IP ranges for webhook server (comma-separated CIDRs)
ALLOWED_IP_RANGES=

# -----------------------------------------------------------------------------
# FEATURE FLAGS
# -----------------------------------------------------------------------------
# Enable automatic issue assignment
ENABLE_AUTO_ASSIGN=true

# Enable parallel agent execution
ENABLE_PARALLEL_EXECUTION=true

# Enable automatic documentation updates
ENABLE_AUTO_DOCUMENTATION=true

# Enable metrics collection
ENABLE_METRICS=true

# Enable webhook mode (false = polling mode)
ENABLE_WEBHOOKS=false

# -----------------------------------------------------------------------------
# DEVELOPMENT / DEBUG
# -----------------------------------------------------------------------------
# Dry run mode (agents don't make real changes)
DRY_RUN=false

# Debug mode (verbose output)
DEBUG=false

# Mock LLM responses (for testing)
MOCK_LLM=false
